{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DEEPRCAN0731.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2LfkJeUqb2l"
      },
      "source": [
        "### ABSTRACT\n",
        "1. UNET의 성능이 생각보다 괜찮은 것 같아서 일단 unet으로 (3, 612, 816) 크기의 이미지를 넣어 그대로 출력하도록 하였기 때문에 맞추어서 예측을 시키고자 한다.\n",
        "2. batch size가 4인 이미지 묶음을 입력한 것은 맞지만 채널수, 입력 이미지의 크기만 맞으면 되기 떄문에 weight를 갱신하는 상황이 아니고서는 이미지의 입력 묶음의 개수는 문제가 되는 부분이 아니다.\n",
        "\n",
        "3. 입력 이미지를 잘라서 넣어주고자 한다, (3, 612, 816)으로 만들어야 하고 입력하는 이미지의 크기는 (3, 1224, 1832)그리고 (3, 2448, 3264)가 존재하기 때문에 이 크기에 맞추어서 crop하는 데이터의 개수를 정해 주어야 한다.\n",
        "\n",
        "4. 우선 훈련용 dataset을 만든 뒤에 학습을 계속 시킬 예정이다.\n",
        "\n",
        "5. 더불어서 학습 시킨 것에 맞게 이미지를 읽어와서 출력값을 하나의 이미지로 합칠 수 있도록 하는 predict함수를 만들어서 이를 제출해 볼 예정이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vyn5LIgTdVk"
      },
      "source": [
        "config = {\n",
        "    'scale' : 1,\n",
        "    'num_features' : 32,\n",
        "    'num_rg' : 7,\n",
        "    'num_rcab' : 3,\n",
        "    'reduction' : 8\n",
        "    }"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wttGo-SE4qKC"
      },
      "source": [
        "import random, os, sys, cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch, math\n",
        "from torch import Tensor\n",
        "from torch.cuda import amp\n",
        "from torch import nn\n",
        "from PIL import Image\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torch.fft as fft\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import albumentations as albu\n",
        "\n",
        "device = torch.device('cuda:0')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJvjGlyt4x12"
      },
      "source": [
        "x_train_path = '/content/drive/MyDrive/DACON/카메라이미지품질향상AI/DATA/IMG/train_input_img'\n",
        "x_label_path = '/content/drive/MyDrive/DACON/카메라이미지품질향상AI/DATA/IMG/train_label_img'\n",
        "train_csv_path = '/content/drive/MyDrive/DACON/카메라이미지품질향상AI/DATA/CSV/train.csv'\n",
        "x_test_path = '/content/drive/MyDrive/DACON/카메라이미지품질향상AI/DATA/IMG/test_input_img'\n",
        "root_path = '/content/drive/MyDrive/DACON/카메라이미지품질향상AI/DATA/RCAN'\n",
        "test_csv_path = '/content/drive/MyDrive/DACON/카메라이미지품질향상AI/DATA/CSV/test.csv'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXl1-tGe40lT"
      },
      "source": [
        "train_csv = pd.read_csv(train_csv_path)\n",
        "\n",
        "train_all_input_files = x_train_path + '/' + train_csv['input_img']\n",
        "train_all_label_files = x_label_path + '/' + train_csv['label_img']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuPz0c3z2M21"
      },
      "source": [
        "test_csv = pd.read_csv(test_csv_path)\n",
        "\n",
        "test_all_input_files = x_test_path + '/' + test_csv['input_img']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSBK7KrB5BbQ"
      },
      "source": [
        "**Functions For Loading and Saving UNet Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VNBMydh46YD"
      },
      "source": [
        "def load(ckpt_dir, net, optim):\n",
        "  if not os.path.exists(ckpt_dir):\n",
        "    epoch = 0\n",
        "    return net, optim, epoch\n",
        "  \n",
        "  net_name = ckpt_dir.split('/')[-2]\n",
        "  \n",
        "  ckpt_lst = os.listdir(ckpt_dir)\n",
        "  ckpt_lst.sort(key = lambda x: int(''.join(filter(str.isdigit, x))))\n",
        "\n",
        "  dict_model = torch.load('%s/%s' %(ckpt_dir, ckpt_lst[-1]))\n",
        "\n",
        "  net.load_state_dict(dict_model[net_name])\n",
        "  optim.load_state_dict(dict_model['optim'])\n",
        "  epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
        "\n",
        "  return net, optim, epoch\n",
        "\n",
        "def save(ckpt_dir, net, optim, epoch):\n",
        "  net_name = ckpt_dir.split('/')[-2]\n",
        "\n",
        "  if not os.path.exists(ckpt_dir):\n",
        "    os.makedirs(ckpt_dir)\n",
        "  \n",
        "  torch.save({net_name:net.state_dict(), 'optim':optim.state_dict()}, '%s/model_epoch%d.pth'%(ckpt_dir, epoch))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEJTa42GIJ1o"
      },
      "source": [
        "def visualize(full_in, full_out, output):\n",
        "  full_in = tonumpy(full_in)\n",
        "  full_out = tonumpy(full_out)\n",
        "  output = tonumpy(output)\n",
        "  plt.figure(1, figsize = (10, 10))\n",
        "  plt.subplot(1, 3, 1)\n",
        "  plt.imshow(full_in)\n",
        "  plt.subplot(1, 3, 2)\n",
        "  plt.imshow(output)\n",
        "  plt.subplot(1, 3, 3)\n",
        "  plt.imshow(full_out)\n",
        "  plt.show()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTui041qQtZG"
      },
      "source": [
        "def psnr(predict, target):\n",
        "    if torch.equal(predict, target):\n",
        "        return 100\n",
        "    else:\n",
        "        mse = torch.mean((predict-target)**2)\n",
        "    return 10 * torch.log10(1/mse)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kq15Gvm13L9"
      },
      "source": [
        "**Utils for VGG Loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdnOI_k017Pc"
      },
      "source": [
        "from collections import namedtuple\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "class VGG16(torch.nn.Module):\n",
        "    def __init__(self, requires_grad = False):\n",
        "        super(VGG16, self).__init__()\n",
        "        vgg_pretrained_features = models.vgg16(pretrained = True).features\n",
        "        self.block1 = torch.nn.Sequential()\n",
        "        self.block2 = torch.nn.Sequential()\n",
        "        self.block3 = torch.nn.Sequential()\n",
        "        self.block4 = torch.nn.Sequential()\n",
        "\n",
        "        for i in range(4):\n",
        "            self.block1.add_module(str(i), vgg_pretrained_features[i])\n",
        "        for i in range(4, 9):\n",
        "            self.block2.add_module(str(i), vgg_pretrained_features[i])\n",
        "        for i in range(9, 16):\n",
        "            self.block3.add_module(str(i), vgg_pretrained_features[i])\n",
        "        for i in range(16, 23):\n",
        "            self.block4.add_module(str(i), vgg_pretrained_features[i])\n",
        "\n",
        "        if requires_grad == False:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.block1(x)\n",
        "        h_relu1_2 = h\n",
        "        h = self.block2(h)\n",
        "        h_relu2_2 = h\n",
        "        h = self.block3(h)\n",
        "        h_relu3_2 = h\n",
        "        h = self.block4(h)\n",
        "        h_relu4_2 = h\n",
        "\n",
        "        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_2', 'relu4_2'])\n",
        "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_2, h_relu4_2)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def gram_matrix(x):\n",
        "    (b, ch, h, w) = x.size()\n",
        "    feat = x.view(b, ch, w*h)\n",
        "    feat_t = feat.transpose(1, 2)\n",
        "    gram = feat.bmm(feat_t) / (ch * w * h)\n",
        "\n",
        "    return gram\n",
        "\n",
        "def normalize(x):\n",
        "    mean = x.new_tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
        "    std = x.new_tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
        "    return (x-mean)/std"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXwumXHc8q_p"
      },
      "source": [
        "#### Function for FFL (Focal Frequency Loss)\n",
        "- torch.fft.fft2를 사용해서 2D discrete Fourier transform을 predicted image와 target image각각에 적용을 시킨다.\n",
        "- 이렇게 변환된 벡터의 절댓값을 (alpha + 2)만큼의 값의 횟수만큼 곱해준다. (이 떄 alpha의 default값은 1) 결국에는 세제곱을 한 뒤에 평균을 내는 것과 마찬가지이다.\n",
        "- m * n 의 크기의 이미지일 경우 해당 크기로 나눈다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78j_qkTg9djb"
      },
      "source": [
        "def FFLLoss(output, target, alpha = 1):\n",
        "    \"\"\"input and target must be a tensor of the image\"\"\"\n",
        "    # fourier transform of the 2D dimension\n",
        "    if (output.dim == 4):\n",
        "        m,n = output.shape[2], output.shape[3]\n",
        "    \n",
        "    # transform dimension = (2, 3)\n",
        "    # normalization (1/sqrt(n))\n",
        "    ft_out = fft.fft2(output, dim = (2, 3), norm = 'ortho').to(device)\n",
        "    ft_tar = fft.fft2(target, dim = (2, 3), norm = 'ortho').to(device)\n",
        "\n",
        "    #ft_out = torch.Tensor(ft_out, requires_grad = True)\n",
        "    #ft_tar = torch.Tensor(ft_tar, requires_grad = True)\n",
        "    f_uv = torch.Tensor(torch.mean((abs(ft_out - ft_tar))**(alpha + 2)))\n",
        "\n",
        "    return f_uv\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmOMJhFBV37X"
      },
      "source": [
        "class LG_Dataset(torch.utils.data.Dataset):\n",
        "    # read and change image size(3, 2448, 3264) -> (3, 306, 408)\n",
        "    def __init__(self, x_img_dirs, y_img_dirs, is_train, split, crop_size = (612, 816), shuffle = True):\n",
        "        # img_dirs는 이미지의 저장 경로가 담겨있는 리스트의 형태\n",
        "        # crop_size는 잘라낼 이미지의 크기로, 일정하게 정해 놓을 예정\n",
        "        self.input = x_img_dirs\n",
        "        self.target = y_img_dirs\n",
        "        self.split = int(len(self.input) * split)\n",
        "        self.w, self.h = crop_size[0], crop_size[1]\n",
        "        \n",
        "\n",
        "        if is_train:\n",
        "          self.input = self.input[:self.split]\n",
        "          self.target = self.target[:self.split]\n",
        "        else:\n",
        "          self.input = self.input[self.split:]\n",
        "          self.target = self.target[self.split:]\n",
        "\n",
        "        self.samples = []\n",
        "        # target, label이미지의 저장 경로를 하나씩 리스트에 넣어줌\n",
        "        for x, y in zip(self.input, self.target):\n",
        "            self.samples.append([x, y])\n",
        "        \n",
        "        if shuffle:\n",
        "          random.shuffle(self.samples)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs, labels = self.samples[idx]\n",
        "        in_img, label_img = pil_image.open(inputs).convert('RGB'), pil_image.open(labels).convert('RGB')\n",
        "        imgtonp = lambda x: np.array(x).astype(np.float32)\n",
        "\n",
        "        w,h = in_img.size[0], in_img.size[1]\n",
        "        # height는 153만큼, width는 204만큼 이동하도록 시킨다\n",
        "        scale_w, scale_h = 153, 204\n",
        "        stride_w, stride_h = 14, 14\n",
        "        \n",
        "        transform = []\n",
        "        transform.append(transforms.Resize((612, 816)))\n",
        "        transform.append(transforms.ToTensor())\n",
        "        transform = transforms.Compose(transform)\n",
        "\n",
        "        start_x, start_y = 0, 0\n",
        "        data = []\n",
        "        resized_x, resized_y = transform(in_img), transform(label_img)\n",
        "        data.append({'input' : resized_x, 'target' : resized_y})\n",
        "\n",
        "        for i in range(stride_h+1):\n",
        "            for j in range(stride_w+1):\n",
        "                end_x, end_y = start_x + 408, start_y + 306\n",
        "                crop_x = in_img.crop((start_x, start_y, end_x, end_y))\n",
        "                crop_y = label_img.crop((start_x, start_y, end_x, end_y))\n",
        "\n",
        "                crop_x = np.array(crop_x).astype(np.float32)\n",
        "                crop_y = np.array(crop_y).astype(np.float32)\n",
        "\n",
        "                # pil_image.read로 읽었기 떄문에 (h, w, 3)이므로\n",
        "                # 입력으로 모델에 넣어 줄 떄에는 (3, h, w)로 바꾸어야 함\n",
        "\n",
        "                # scale 0-255.0 -> 0-1\n",
        "                crop_x = crop_x/255.0\n",
        "                crop_y = crop_y/255.0\n",
        "\n",
        "                crop_x = transforms.ToTensor()(crop_x)\n",
        "                crop_y = transforms.ToTensor()(crop_y)\n",
        "\n",
        "                data.append({'input' : crop_x, 'target' : crop_y})\n",
        "                start_x += scale_h\n",
        "            start_y += scale_w\n",
        "            start_x = 0\n",
        "        #data = random.shuffle(data)\n",
        "        return data    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "630Q2nOup2LU"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data.dataset as Dataset\n",
        "import numpy as np\n",
        "import PIL.Image as pil_image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "class CroppedDataset(torch.utils.data.Dataset):\n",
        "    # read and change image size(3, 2448, 3264) -> (3, 612, 816)\n",
        "    def __init__(self, x_img_dirs, y_img_dirs, split, is_train, crop_size = (612, 816)):\n",
        "        # img_dirs는 이미지의 저장 경로가 담겨있는 리스트의 형태\n",
        "        # crop_size는 잘라낼 이미지의 크기로, 일정하게 정해 놓을 예정\n",
        "        self.input = x_img_dirs\n",
        "        self.target = y_img_dirs\n",
        "        self.split = int(len(self.input) * split)\n",
        "        self.w, self.h = crop_size[0], crop_size[1]\n",
        "\n",
        "        if is_train:\n",
        "          self.input = self.input[:self.split]\n",
        "          self.target = self.target[:self.split]\n",
        "        else:\n",
        "          self.input = self.input[self.split:]\n",
        "          self.target = self.target[self.split:]\n",
        "\n",
        "        self.samples = []\n",
        "        # target, label이미지의 저장 경로를 하나씩 리스트에 넣어줌\n",
        "        for x, y in zip(self.input, self.target):\n",
        "            self.samples.append([x, y])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs, labels = self.samples[idx]\n",
        "        in_img, label_img = pil_image.open(inputs).convert('RGB'), pil_image.open(labels).convert('RGB')\n",
        "        imgtonp = lambda x: np.array(x).astype(np.float32)\n",
        "        \n",
        "        transform_1, transform_2 = [], []\n",
        "        transform_1.append(transforms.Resize((self.w, self.h)))\n",
        "        #transform_1.append(transforms.RandomHorizontalFlip())\n",
        "        #transform_1.append(transforms.RandomRotation(degrees = 90))\n",
        "        transform_1.append(transforms.RandomHorizontalFlip(p = 1))\n",
        "        transform_1.append(transforms.ToTensor())\n",
        "        transform_1 = transforms.Compose(transform_1)\n",
        "\n",
        "        transform_2.append(transforms.RandomVerticalFlip(p = 1))\n",
        "        transform_2.append(transforms.ToTensor())\n",
        "        transform_2 = transforms.Compose(transform_2)\n",
        "\n",
        "        resized_x = transform_1(in_img)\n",
        "        resized_y = transform_1(label_img)\n",
        "\n",
        "        h, w = in_img.size[0], in_img.size[1]\n",
        "\n",
        "        scale = int(w/self.w)\n",
        "        img_nums = scale ** 2\n",
        "\n",
        "        start_x, start_y = 0, 0\n",
        "        data = []\n",
        "        data.append({'input' : resized_x, 'target' : resized_y})\n",
        "        for i in range(1, scale+1):\n",
        "            for j in range(1, scale+1):\n",
        "                end_x, end_y = start_x + self.h, start_y + self.w\n",
        "                crop_x = in_img.crop((start_x, start_y, end_x, end_y))\n",
        "                crop_y = label_img.crop((start_x, start_y, end_x, end_y))\n",
        "\n",
        "                crop_x = transforms.RandomVerticalFlip(p = 1)(crop_x)\n",
        "                crop_y = transforms.RandomVerticalFlip(p = 1)(crop_y)\n",
        "\n",
        "                crop_x = np.array(crop_x).astype(np.float32)\n",
        "                crop_y = np.array(crop_y).astype(np.float32)\n",
        "\n",
        "                # pil_image.read로 읽었기 떄문에 (h, w, 3)이므로\n",
        "                # 입력으로 모델에 넣어 줄 떄에는 (3, h, w)로 바꾸어야 함\n",
        "\n",
        "                # scale 0-255.0 -> 0-1\n",
        "                crop_x = crop_x/255.0\n",
        "                crop_y = crop_y/255.0\n",
        "\n",
        "                crop_x = transforms.ToTensor()(crop_x)\n",
        "                crop_y = transforms.ToTensor()(crop_y)\n",
        "\n",
        "                data.append({'input' : crop_x, 'target' : crop_y})\n",
        "                start_x += self.h\n",
        "            start_y += self.w\n",
        "            start_x = 0\n",
        "\n",
        "        return data           "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLL1EwFp67Mw"
      },
      "source": [
        "class SlidingWindowDataset(torch.utils.data.Dataset):\n",
        "    # read and change image size(3, 2448, 3264) -> (3, 612, 816)\n",
        "    def __init__(self, x_img_dirs, y_img_dirs, split, is_train, crop_size = (612, 816)):\n",
        "        # img_dirs는 이미지의 저장 경로가 담겨있는 리스트의 형태\n",
        "        # crop_size는 잘라낼 이미지의 크기로, 일정하게 정해 놓을 예정\n",
        "        self.input = x_img_dirs\n",
        "        self.target = y_img_dirs\n",
        "        self.split = int(len(self.input) * split)\n",
        "        self.w, self.h = crop_size[0], crop_size[1]\n",
        "        \n",
        "        self.crop_rate = 4 # 2의 제곱수, 2, 4, 8\n",
        "        self.last = self.crop_rate - 2\n",
        "        \n",
        "\n",
        "        if is_train:\n",
        "          self.input = self.input[:self.split]\n",
        "          self.target = self.target[:self.split]\n",
        "        else:\n",
        "          self.input = self.input[self.split:]\n",
        "          self.target = self.target[self.split:]\n",
        "\n",
        "        self.samples = []\n",
        "        # target, label이미지의 저장 경로를 하나씩 리스트에 넣어줌\n",
        "        for x, y in zip(self.input, self.target):\n",
        "            self.samples.append([x, y])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs, labels = self.samples[idx]\n",
        "        in_img, label_img = pil_image.open(inputs).convert('RGB'), pil_image.open(labels).convert('RGB')\n",
        "        imgtonp = lambda x: np.array(x).astype(np.float32)\n",
        "        \n",
        "        transform = []\n",
        "        transform.append(transforms.Resize((self.w, self.h)))\n",
        "        transform.append(transforms.ToTensor())\n",
        "        transform = transforms.Compose(transform)\n",
        "\n",
        "        resized_x = transform(in_img)\n",
        "        resized_y = transform(label_img)\n",
        "\n",
        "        h, w = in_img.size[0], in_img.size[1]\n",
        "\n",
        "        # scale = int(w/self.w)\n",
        "        # img_nums = scale ** 2\n",
        "\n",
        "        stride_h = h//(4*self.crop_rate)\n",
        "        stride_w = w//(4*self.crop_rate)\n",
        "        scale_h = h//stride_h\n",
        "        scale_w = w//stride_w\n",
        "\n",
        "        start_x, start_y = 0, 0\n",
        "        data = []\n",
        "        data.append({'input' : resized_x, 'target' : resized_y})\n",
        "        for i in range(1, scale_w-self.last):\n",
        "            for j in range(1, scale_h-self.last):\n",
        "                end_x, end_y = start_x + self.h, start_y + self.w\n",
        "                crop_x = in_img.crop((start_x, start_y, end_x, end_y))\n",
        "                crop_y = label_img.crop((start_x, start_y, end_x, end_y))\n",
        "\n",
        "                crop_x = np.array(crop_x).astype(np.float32)\n",
        "                crop_y = np.array(crop_y).astype(np.float32)\n",
        "\n",
        "                # pil_image.read로 읽었기 떄문에 (h, w, 3)이므로\n",
        "                # 입력으로 모델에 넣어 줄 떄에는 (3, h, w)로 바꾸어야 함\n",
        "\n",
        "                # scale 0-255.0 -> 0-1\n",
        "                crop_x = crop_x/255.0\n",
        "                crop_y = crop_y/255.0\n",
        "\n",
        "                crop_x = transforms.ToTensor()(crop_x)\n",
        "                crop_y = transforms.ToTensor()(crop_y)\n",
        "\n",
        "                data.append({'input' : crop_x, 'target' : crop_y})\n",
        "                start_x += self.h\n",
        "                # start_x += scale_h\n",
        "            # start_y += self.w\n",
        "            start_y += scale_w\n",
        "            start_x = 0\n",
        "\n",
        "        return data    "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrwnXt_l6_OZ"
      },
      "source": [
        "**RCAN Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiWRLMNo69az"
      },
      "source": [
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, num_features, reduction):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.module = nn.Sequential(\n",
        "            # 각 채널별로 average pooling을 진행한다 (채널 수가 c개이면 c개 모두에 대해서 진행)\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            # downsampling                                             \n",
        "            nn.Conv2d(num_features, num_features // reduction, kernel_size=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # upsampling\n",
        "            nn.Conv2d(num_features // reduction, num_features, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.module(x)\n",
        "\n",
        "\n",
        "class RCAB(nn.Module):\n",
        "  # RIR(Residual in Residual) 구조를 적용\n",
        "    def __init__(self, num_features, reduction):\n",
        "        super(RCAB, self).__init__()\n",
        "        self.module = nn.Sequential(\n",
        "            nn.Conv2d(num_features, num_features, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(num_features, num_features, kernel_size=3, padding=1),\n",
        "            ChannelAttention(num_features, reduction)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "      # x는 RCAB의 input, output과 element-wise sum을 적용\n",
        "      return x + self.module(x)\n",
        "\n",
        "\n",
        "class RG(nn.Module):\n",
        "  # RG(Residual Group)\n",
        "    def __init__(self, num_features, num_rcab, reduction):\n",
        "        super(RG, self).__init__()\n",
        "        # num_recab = 12(하나의 RG안에 RCAB이 12개 존재)\n",
        "        self.module = [RCAB(num_features, reduction) for _ in range(num_rcab)]\n",
        "        self.module.append(nn.Conv2d(num_features, num_features, kernel_size=3, padding=1))\n",
        "        self.module = nn.Sequential(*self.module)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.module(x)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzrszZ_PSubJ"
      },
      "source": [
        "class RCAN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(RCAN, self).__init__()\n",
        "        scale = config['scale']\n",
        "        num_features = config['num_features']\n",
        "        # number of residual groups\n",
        "        num_rg = config['num_rg']\n",
        "        num_rcab = config['num_rcab']\n",
        "        reduction = config['reduction']\n",
        "        \n",
        "        # 먼저 low resolution의 이미지를 4배로 늘려주어 적용시킨다.\n",
        "        self.deconv = nn.ConvTranspose2d(3, 3, kernel_size=9, stride=4, padding=4, output_padding=3)\n",
        "        # 본격적인 특성 추출에 앞서서 표면적인 특성을 추출하는 하나의 convolution layer\n",
        "        self.sf = nn.Conv2d(3, num_features, kernel_size=3, padding=1)\n",
        "        # RIR, 즉 residual in residual모듈을 적용함\n",
        "        self.rgs = nn.Sequential(*[RG(num_features, num_rcab, reduction) for _ in range(num_rg)])\n",
        "        self.conv1 = nn.Conv2d(num_features, num_features, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(num_features, 3, kernel_size=3, padding=1)\n",
        "        # 0.0 ~ 1.0 사이의 픽셀값을 갖는 이미지의 출력을 위해 sigmoid적용\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.deconv(x)\n",
        "        x = self.sf(x)\n",
        "        residual = x\n",
        "        x = self.rgs(x)\n",
        "        x = self.conv1(x)\n",
        "        x += residual\n",
        "        x = self.conv2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4uoqjte9dfg"
      },
      "source": [
        "- BATCH_SIZE를 1로 설정해 줌으로서 한번에 resize된 전체 이미지와 잘린 이미지의 데이터들을 받는다.\n",
        "- 안그러면 이미지를 학습 시킬 떄에 오래 걸림.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9K-hPiDDBK0"
      },
      "source": [
        "model = RCAN(config)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXvEG2D_9D5S"
      },
      "source": [
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 1\n",
        "NUM_EPOCH = 100\n",
        "FULLNET_CKPT_DIR = '/content/drive/MyDrive/DACON/카메라이미지품질향상AI/DATA/models/DEEPRCAN/L1Loss'\n",
        "\n",
        "# loss function\n",
        "fn_loss = nn.L1Loss().to(device)\n",
        "\n",
        "# optimizer\n",
        "optim = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE, betas = [0.9, 0.999], eps = 10**-8)\n",
        "\n",
        "model, optim, start_epoch = load(ckpt_dir = FULLNET_CKPT_DIR, net= model, optim = optim)\n",
        "\n",
        "#train_cd = LG_Dataset(train_all_input_files, train_all_label_files, split = 0.7, is_train = True, shuffle = True)\n",
        "#val_cd = LG_Dataset(train_all_input_files, train_all_label_files, split = 0.7, is_train = False, shuffle = True)\n",
        "\n",
        "train_cd = CroppedDataset(train_all_input_files, train_all_label_files, split = 1, is_train = True)\n",
        "val_cd = CroppedDataset(train_all_input_files, train_all_label_files, split = 0.6, is_train = False)\n",
        "\n",
        "#train_cd = SlidingWindowDataset(train_all_input_files, train_all_label_files, split = 0.7, is_train = True)\n",
        "#val_cd = SlidingWindowDataset(train_all_input_files, train_all_label_files, split = 0.7, is_train = False)\n",
        "\n",
        "num_train, num_val = len(train_cd), len(val_cd)\n",
        "\n",
        "num_train_for_epoch, num_val_for_epoch = np.ceil(num_train/BATCH_SIZE), np.ceil(num_val/BATCH_SIZE)\n",
        "\n",
        "train_writer = SummaryWriter(log_dir = os.path.join(root_path, 'train'))\n",
        "val_writer = SummaryWriter(log_dir =  os.path.join(root_path, 'val'))\n",
        "\n",
        "train_loader = DataLoader(train_cd, batch_size = BATCH_SIZE, shuffle = True)\n",
        "val_loader = DataLoader(val_cd, batch_size = BATCH_SIZE, shuffle = True)\n",
        "\n",
        "tonumpy = lambda x : x.cpu().detach().numpy().squeeze(0).transpose(1, 2, 0)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viB2fqj7-IOK"
      },
      "source": [
        "start_epoch = 0\n",
        "num_epoch = 50\n",
        "alpha = 0.5\n",
        "\n",
        "model, optim, start_epoch = load(ckpt_dir = FULLNET_CKPT_DIR, net = model, optim = optim)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=10, eta_min=0)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "count = 0\n",
        "for epoch in range(start_epoch + 1, num_epoch + 1+start_epoch):\n",
        "  model.train()\n",
        "  loss_arr = []\n",
        "  best_psnr = 0\n",
        "\n",
        "  for batch, data in enumerate(train_loader, 1):\n",
        "    psnr_score = 0\n",
        "    # forward\n",
        "    count += 1\n",
        "    full_in, full_out = data[0]['input'].to(device), data[0]['target'].to(device)\n",
        "    output = model(full_in)\n",
        "    optim.zero_grad()\n",
        "\n",
        "    loss = fn_loss(output, full_out)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    scheduler.step()\n",
        "    loss_arr += [loss.item()]\n",
        "    psnr_score = psnr(output, full_out)\n",
        "    if batch%50 == 0:\n",
        "      print('train : epoch %f/ %f | Batch %f/ %f | Loss %f  | PSNR %f'%(epoch, num_epoch+start_epoch, batch, num_train_for_epoch, loss, psnr_score))\n",
        "      visualize(full_in, full_out, output)\n",
        "    if psnr_score > best_psnr == 0:\n",
        "      save(ckpt_dir = FULLNET_CKPT_DIR, net = model, optim = optim, epoch = count)\n",
        "      best_psnr = psnr_score\n",
        "\n",
        "    for i in range(1, len(data)):\n",
        "      # count += 1\n",
        "      # save(ckpt_dir = FULLNET_CKPT_DIR, net = model, optim = optim, epoch = count)\n",
        "      in_img = data[i]['input'].to(device)\n",
        "      label_img = data[i]['target'].to(device)\n",
        "      output = model(in_img)\n",
        "      loss = fn_loss(output, label_img)\n",
        "      if loss > 0.0005:\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        scheduler.step()\n",
        "        loss_arr += [loss.item()]\n",
        "    \n",
        "  train_writer.add_scalar('loss', np.mean(loss_arr), epoch)\n",
        "  \n",
        "  # validation test\n",
        "  # validation dataset을 이용해서 학습의 정확도를 판단하고자 하는 상황이기 때문에 backprop진행을 안함\n",
        "  with torch.no_grad() :\n",
        "    model.eval()\n",
        "    loss_arr = []\n",
        "\n",
        "    for batch, data in enumerate(val_loader, 1):\n",
        "    # forward\n",
        "      full_in, full_out = data[0]['input'].to(device), data[0]['target'].to(device)\n",
        "      output = model(full_in)\n",
        "      loss = fn_loss(output, label_img)\n",
        "      loss_arr += [loss.item()]\n",
        "      psnr_score = psnr(output, full_in)\n",
        "      print('valid : epoch %f/ %f | Batch %f/ %f | Loss %f  | PSNR %f'%(epoch, num_epoch+start_epoch, batch, num_train_for_epoch, loss, psnr_score))\n",
        "      if batch%10 == 0:\n",
        "        visualize(full_in, full_out, output)\n",
        "\n",
        "      for i in range(1, len(data)):\n",
        "        in_img = data[i]['input'].to(device)\n",
        "        label_img = data[i]['target'].to(device)\n",
        "        output = model(in_img)\n",
        "        #feat_x, feat_y = vgg(output), vgg(label_img)\n",
        "        #loss = L1_loss(feat_x.relu2_2, feat_y.relu2_2)\n",
        "        loss = fn_loss(output, label_img)\n",
        "        loss_arr += [loss.item()]\n",
        "\n",
        "    val_writer.add_scalar('loss', np.mean(loss_arr), epoch) \n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCOxLG6Esl8A"
      },
      "source": [
        "save(ckpt_dir = FULLNET_CKPT_DIR, net = model, optim = optim, epoch = count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z74hKLpSAcNp"
      },
      "source": [
        "model, optim, start_epoch = load(ckpt_dir = FULLNET_CKPT_DIR, net = model, optim = optim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEmS6iMFLG7z"
      },
      "source": [
        "CKPT_DIR = '/content/drive/MyDrive/DACON/카메라이미지품질향상AI/DATA/models/DEEPRCAN/L1LOSS'\n",
        "#model = RCAN(config)\n",
        "#optim = torch.optim.Adam(model.parameters(), lr = 1e-4, betas = [0.9, 0.999], eps = 10**-8)\n",
        "#model, optim, start_epoch = load(ckpt_dir = CKPT_DIR, net = model, optim = optim)\n",
        "#model = model.to(device)\n",
        "\n",
        "import PIL.Image as pil_image\n",
        "\n",
        "def predict(img_dirs, model):\n",
        "    results = []\n",
        "    for i in range(len(img_dirs)):\n",
        "        img = pil_image.open(img_dirs[i]).convert('RGB')\n",
        "        print(img.size)\n",
        "        w,h = img.size[0], img.size[1] # (h, w)\n",
        "        plt.imshow(img)\n",
        "        plt.show()\n",
        "        scale = h//612\n",
        "        start_x, start_y = 0,0\n",
        "        result_img = np.zeros((h, w, 3))\n",
        "        #print(result_img.shape)\n",
        "        channel = 0\n",
        "        for j in range(1, scale+1):\n",
        "            for k in range(1, scale+1):\n",
        "                end_x, end_y = start_x + 816, start_y + 612\n",
        "                crop_img = img.crop((start_x, start_y, end_x, end_y))\n",
        "                crop_img = np.array(crop_img).astype(np.float32)/255.0\n",
        "                crop_img = transforms.ToTensor()(crop_img)\n",
        "                pred = model(torch.unsqueeze(crop_img, 0).to(device))\n",
        "                #print(pred.shape)\n",
        "                pred = pred[0, :, :, :].to('cpu').detach().permute(1, 2, 0).numpy()\n",
        "                #plt.imshow(pred)\n",
        "                #plt.show()\n",
        "                #print(pred.shape)\n",
        "                for c in range(3):\n",
        "                    result_img[start_y:start_y + 612, start_x:start_x + 816, c] = pred[:, :, c]\n",
        "                start_x += 816\n",
        "            start_y += 612\n",
        "            start_x = 0\n",
        "        plt.imshow(result_img)\n",
        "        plt.show()\n",
        "        results.append(result_img)\n",
        "\n",
        "    return results\n",
        "\n",
        "def make_submission(result):\n",
        "    import zipfile\n",
        "    os.makedirs('submission', exist_ok = True)\n",
        "    sub_imgs = []\n",
        "    for i, img in enumerate(result):\n",
        "        img = (img * 255).astype(np.uint8)\n",
        "        path = f'test_{20000+i}.png'\n",
        "        cv2.imwrite(path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "        sub_imgs.append(path)\n",
        "    submission = zipfile.ZipFile('submission.zip', 'w')\n",
        "    for path in sub_imgs:\n",
        "        submission.write(path)\n",
        "    submission.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kU5jTwXzHpM"
      },
      "source": [
        "result = predict(test_all_input_files, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AGhmO5VzP7S"
      },
      "source": [
        "make_submission(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KyiruLIvrbD"
      },
      "source": [
        "import PIL.Image as pil_image\n",
        "import zipfile\n",
        "\n",
        "def predict_means(img_dirs, model):\n",
        "    #crop_rate = 4 # 2의 제곱수, 2, 4, 8\n",
        "    #last = crop_rate - 2\n",
        "    scale_h, scale_w = 153, 204\n",
        "    results = []\n",
        "    for i in range(len(img_dirs)):\n",
        "        # position = []\n",
        "        img = pil_image.open(img_dirs[i]).convert('RGB')\n",
        "        #print(img.size)\n",
        "        w,h = img.size[0], img.size[1] # (h, w)\n",
        "        plt.imshow(img)\n",
        "        plt.show()\n",
        "\n",
        "        #stride_h = h//(4*crop_rate)\n",
        "        #stride_w = w//(4*crop_rate)\n",
        "        stride_h = (2448-612)//scale_h\n",
        "        stride_w = (3264-816)//scale_w\n",
        "\n",
        "        start_x, start_y = 0,0\n",
        "        result_img = np.zeros((h, w, 3))\n",
        "        voting_mask = np.zeros((h, w, 3))\n",
        "\n",
        "        # 마지막 이미지 제외하고 iteration\n",
        "        for i in range(13): # height, y\n",
        "            for j in range(13): # width, x\n",
        "                end_x, end_y = start_x + 816, start_y + 612\n",
        "                # position.append([start_x, start_y])\n",
        "                crop_img = img.crop((start_x, start_y, end_x, end_y))\n",
        "                crop_img = np.array(crop_img).astype(np.float32)\n",
        "                crop_img = crop_img/255.0\n",
        "                crop_img = transforms.ToTensor()(crop_img)\n",
        "\n",
        "                pred = model(torch.unsqueeze(crop_img, 0).to(device))*255\n",
        "                pred = pred[0, :, :, :].to('cpu').detach().permute(1, 2, 0).numpy()\n",
        "\n",
        "                #print(pred.shape)\n",
        "                #plt.imshow(pred)\n",
        "                #plt.show()\n",
        "                #result_img[start_y:start_y + 612, start_x:start_x + 816, :] += pred[:, :]\n",
        "                #voting_mask[start_y:start_y + 612, start_x:start_x + 816, :] += 1\n",
        "                \n",
        "\n",
        "                for c in range(3):\n",
        "                    result_img[start_y:start_y + 612, start_x:start_x + 816, c] += pred[:, :, c]\n",
        "                    voting_mask[start_y:start_y + 612, start_x:start_x + 816, c] += 1\n",
        "                start_x += scale_w\n",
        "            start_y += scale_h\n",
        "            start_x = 0\n",
        "\n",
        "        result_img = result_img/voting_mask\n",
        "        #print(result_img)\n",
        "        result_img = result_img.astype(np.uint8)\n",
        "        plt.imshow(result_img)\n",
        "        plt.show()\n",
        "\n",
        "        results.append(result_img)\n",
        "    return results\n",
        "\n",
        "def make_mean_submission(result):\n",
        "    import zipfile\n",
        "    os.makedirs('submission', exist_ok = True)\n",
        "    sub_imgs = []\n",
        "    for i, img in enumerate(result):\n",
        "        img = (img).astype(np.uint8)\n",
        "        path = f'test_{20000+i}.png'\n",
        "        #cv2.imwrite(path, img)\n",
        "        cv2.imwrite(path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "        sub_imgs.append(path)\n",
        "    submission = zipfile.ZipFile('submission.zip', 'w')\n",
        "    for path in sub_imgs:\n",
        "        submission.write(path)\n",
        "    submission.close()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-N69HmiIfHl"
      },
      "source": [
        "mean_result = predict_means(test_all_input_files, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzWBvgfryLkA"
      },
      "source": [
        "make_mean_submission(mean_result)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "091t5Pgz0I5j"
      },
      "source": [
        "import PIL.Image as pil_image\n",
        "import zipfile\n",
        "\n",
        "def largemean_predict(img_dirs, model):\n",
        "    crop_rate = 4 # 2의 제곱수, 2, 4, 8\n",
        "    last = crop_rate - 2\n",
        "    results = []\n",
        "    for i in range(len(img_dirs)):\n",
        "        # position = []\n",
        "        img = pil_image.open(img_dirs[i]).convert('RGB')\n",
        "        #print(img.size)\n",
        "        w,h = img.size[0], img.size[1] # (h, w)\n",
        "        plt.imshow(img)\n",
        "        plt.show()\n",
        "\n",
        "        stride_h = h//(4*crop_rate)\n",
        "        stride_w = w//(4*crop_rate)\n",
        "        scale_h = h//stride_h\n",
        "        scale_w = w//stride_w\n",
        "        start_x, start_y = 0,0\n",
        "        result_img = np.zeros((h, w, 3))\n",
        "        voting_mask = np.zeros((h, w, 3))\n",
        "\n",
        "        # 마지막 이미지 제외하고 iteration\n",
        "        for i in range(1, scale_h-last): # height, y\n",
        "            for j in range(1, scale_w-last): # width, x\n",
        "                end_x, end_y = start_x + 816, start_y + 612\n",
        "                # position.append([start_x, start_y])\n",
        "                crop_img = img.crop((start_x, start_y, end_x, end_y))\n",
        "                crop_img = np.array(crop_img).astype(np.float32)\n",
        "                crop_img = crop_img/255.0\n",
        "                crop_img = transforms.ToTensor()(crop_img)\n",
        "\n",
        "                pred = model(torch.unsqueeze(crop_img, 0).to(device))*255\n",
        "                pred = pred[0, :, :, :].to('cpu').detach().permute(1, 2, 0).numpy()\n",
        "\n",
        "                #print(pred.shape)\n",
        "                #plt.imshow(pred)\n",
        "                #plt.show()\n",
        "                #result_img[start_y:start_y + 612, start_x:start_x + 816, :] += pred[:, :]\n",
        "                #voting_mask[start_y:start_y + 612, start_x:start_x + 816, :] += 1\n",
        "                \n",
        "\n",
        "                for c in range(3):\n",
        "                    result_img[start_y:start_y + 612, start_x:start_x + 816, c] += pred[:, :, c]\n",
        "                    voting_mask[start_y:start_y + 612, start_x:start_x + 816, c] += 1\n",
        "                start_x += stride_w\n",
        "            start_y += stride_h\n",
        "            start_x = 0\n",
        "\n",
        "        result_img = result_img/voting_mask\n",
        "        #print(result_img)\n",
        "        result_img = result_img.astype(np.uint8)\n",
        "        plt.imshow(result_img)\n",
        "        plt.show()\n",
        "\n",
        "        results.append(result_img)\n",
        "    return results"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaZAmwnX0qpg"
      },
      "source": [
        "largemean_result = largemean_predict(test_all_input_files, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JClew9mc0wQv"
      },
      "source": [
        "make_mean_submission(largemean_result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}